{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7219b37-d23f-4dcf-b1d6-6ee91f20afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import face_alignment\n",
    "from continuous_landmarks.dataset.transforms import (\n",
    "    Compose, Align, Resize,\n",
    "    CenterCrop, AbsToRelLdmks,\n",
    "    ToTensor, Normalize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000462ea-7192-4f26-89e8-22afc98ceb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(video_path):\n",
    "    frames = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = frame[..., ::-1]\n",
    "        frames.append(frame)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f140dd13-0b26-4182-92b8-590a57b548a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from face_alignment.utils import crop, get_preds_fromhm\n",
    "\n",
    "\n",
    "def get_face_detections(frames):\n",
    "    face_detections = []\n",
    "    \n",
    "    for frame in tqdm(frames):\n",
    "        dets = fa.face_detector.detect_from_image(frame.copy())\n",
    "        assert len(dets) == 1\n",
    "        face_detections.append(dets[0])\n",
    "\n",
    "    return face_detections\n",
    "\n",
    "\n",
    "def get_detection_center_scale(detection):\n",
    "    d = detection\n",
    "    center = torch.tensor(\n",
    "        [d[2] - (d[2] - d[0]) / 2.0, d[3] - (d[3] - d[1]) / 2.0]\n",
    "    )\n",
    "    center[1] = center[1] - (d[3] - d[1]) * 0.12\n",
    "    scale = (d[2] - d[0] + d[3] - d[1]) / fa.face_detector.reference_scale\n",
    "    return center, scale\n",
    "\n",
    "\n",
    "def crop_face(frame, center, scale):\n",
    "    inp = crop(frame, center, scale)\n",
    "    inp = torch.from_numpy(inp.transpose((2, 0, 1))).float()\n",
    "    inp.div_(255.0)\n",
    "\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86da523-1409-499f-82f2-70c97aac6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(frames, face_detections, device='cuda'):\n",
    "    det_center_scales = [\n",
    "        get_detection_center_scale(d)\n",
    "        for d in face_detections\n",
    "    ]\n",
    "\n",
    "    face_crops = torch.stack([\n",
    "        crop_face(frame, *cs) for frame, cs in zip(frames, det_center_scales)\n",
    "    ]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_batch = fa.face_alignment_net(face_crops).cpu().numpy()\n",
    "\n",
    "    landmarks = []\n",
    "\n",
    "    for out, (center, scale) in zip(out_batch, det_center_scales):\n",
    "        pts, pts_img, scores = get_preds_fromhm(out[None, ...], center.numpy(), scale)\n",
    "        pts, pts_img = torch.from_numpy(pts), torch.from_numpy(pts_img)\n",
    "        pts, pts_img = pts.view(68, 2) * 4, pts_img.view(68, 2)\n",
    "        landmarks.append(pts_img)\n",
    "\n",
    "    return torch.stack(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74eca51-8dbf-4a15-ac66-4bc6a6766f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_video_frames('test.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6709f23e-5103-465c-b53d-db22dc646067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "lm_cache_path = Path('landmarks_cache.pth')\n",
    "LAZY = True\n",
    "\n",
    "if not lm_cache_path.exists() or not LAZY:\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)\n",
    "    face_detections = get_face_detections(frames)\n",
    "    landmarks = get_landmarks(frames, face_detections)\n",
    "    torch.save(landmarks, lm_cache_path)\n",
    "else:\n",
    "    landmarks = torch.load(lm_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c891c-d661-4669-8c4a-bc5c18ce6ad0",
   "metadata": {},
   "source": [
    "# Continuous landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9130e0f6-fe2d-4ad9-9038-f8b852a2879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eyes_mouth(points):\n",
    "    assert len(points) == 68\n",
    "\n",
    "    e0 = points[36:42].mean(axis=0)\n",
    "    e1 = points[42:48].mean(axis=0)\n",
    "    m0 = points[48]\n",
    "    m1 = points[54]\n",
    "\n",
    "    return e0, e1, m0, m1\n",
    "\n",
    "tfm = Compose([\n",
    "    Align(get_eyes_mouth),\n",
    "    Resize(224),\n",
    "    CenterCrop(224),\n",
    "    AbsToRelLdmks(),\n",
    "    ToTensor(),\n",
    "    Normalize([.5, .5, .5], [.2, .2, .2]),\n",
    "])\n",
    "\n",
    "frame_batch, lm_batch = list(zip(*[tfm(Image.fromarray(frame), np.array(lms)) for frame, lms in zip(frames, landmarks)]))\n",
    "frame_batch = torch.stack(frame_batch)\n",
    "lm_batch = torch.stack(lm_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a9cb60a-3482-45cf-baec-ce9ee995c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuous_landmarks.model import FeatureExtractor, LandmarkPredictor, \\\n",
    "    PositionEncoder\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "pos_encoder = PositionEncoder()\n",
    "feat_extractor = FeatureExtractor('MobileNetV3')\n",
    "lm_predictor = LandmarkPredictor(\n",
    "    query_size=pos_encoder.encoding_size,\n",
    "    feature_size=feat_extractor.feature_size,\n",
    "    model_name='MLP',\n",
    ")\n",
    "\n",
    "model = nn.ModuleDict({\n",
    "    'PositionEncoder': pos_encoder,\n",
    "    'FeatureExtractor': feat_extractor,\n",
    "    'LandmarkPredictor': lm_predictor\n",
    "})\n",
    "model.load_state_dict(torch.load('../ckpts/95yvg3pn_best.pth'))\n",
    "model.to(device).eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbf64ae-9c82-428c-9a40-015d5f8273a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "canonical_shape = torch.load('../continuous_landmarks/dataset/facescape_mouth_stretch.pth').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6844845b-851b-4390-9c2e-62f103b75fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "frame_batches = [frame_batch[i:i + batch_size] for i in range(0, len(frame_batch), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc60c923-5352-4311-81c2-301becd60357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from continuous_landmarks.training.training_loop import get_inv_tfm\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "inv_tfm = get_inv_tfm(tfm)\n",
    "\n",
    "input_ims = []\n",
    "pred_lms = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in frame_batches:\n",
    "        canon_batch = canonical_shape[None, ...].expand(len(batch), -1, -1)\n",
    "        B, N, _ = canon_batch.shape\n",
    "        query_sequence = pos_encoder(\n",
    "            canon_batch.flatten(end_dim=1)\n",
    "        ).unflatten(0, (B, N))\n",
    "        feature = feat_extractor(batch.to(device))\n",
    "        lm_pred, var_pred = lm_predictor(query_sequence, feature)\n",
    "\n",
    "        for img, lms in zip(batch, lm_pred):\n",
    "            img, lms = inv_tfm(img.cpu(), lms.cpu())\n",
    "            input_ims.append(to_pil_image(img))\n",
    "            pred_lms.append(lms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e5ef172-00d1-41d7-b6a1-88dd56f0a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [00:05<00:00, 31.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from continuous_landmarks.utils.draw_points import draw_points\n",
    "from continuous_landmarks.utils.face_alignment import get_matrix_and_size\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, 30.0, frames[0].shape[1::-1])\n",
    "\n",
    "for orig_img, align_lms, input_im, lms in zip(tqdm(frames), landmarks, input_ims, pred_lms):\n",
    "    eye_0, eye_1, mouth_0, mouth_1 = get_eyes_mouth(align_lms)\n",
    "    M, align_size = get_matrix_and_size(\n",
    "        eye_0, eye_1, mouth_0, mouth_1\n",
    "    )\n",
    "\n",
    "    # Undo resize\n",
    "    assert input_im.width / input_im.height == 1\n",
    "    input_size = input_im.width\n",
    "    orig_height, orig_width, _ = orig_img.shape\n",
    "    orig_size = min(orig_height, orig_width)\n",
    "    lms = lms * align_size / input_size\n",
    "\n",
    "    # Undo alignment\n",
    "    lms = lms @ np.linalg.inv(M[:, :2]).T - M[:, 2]\n",
    "\n",
    "    im = draw_points(orig_img, lms[::10], size=1)\n",
    "    out.write(np.array(im)[..., ::-1])\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f3cc7-1fc2-4f6e-9a9a-51a53fa4736b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
